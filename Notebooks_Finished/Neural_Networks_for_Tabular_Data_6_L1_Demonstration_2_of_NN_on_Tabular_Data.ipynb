{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<a href=\"https://colab.research.google.com/github/aurimas13/CodeAcademy-AI-Course/blob/main/Notebooks_Finished/Neural_Networks_for_Tabular_Data_6_L1_Demonstration_2_of_NN_on_Tabular_Data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ],
      "metadata": {
        "id": "6vFjkBSfMnf3"
      },
      "id": "6vFjkBSfMnf3"
    },
    {
      "cell_type": "markdown",
      "id": "693267ce",
      "metadata": {
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "693267ce"
      },
      "source": [
        "The use of deep learning in tabular data was not a new idea. People have been using fully-connected neural networks for these data for the longest time but it came with a few disadvantages;\n",
        "\n",
        "Large amount of data needed for neural networks\n",
        "Insignificant gains in model’s performance compared to more popular machine learning algorithms (e.g Random forest, Gradient boosting trees)\n",
        "Lack of interpretability\n",
        "As such, using deep learning for tabular data never really took off. This is true until practitioners transfer the idea of embedding networks used in Natural Language Processing (NLP) to tabular data.\n",
        "\n",
        "This is how it works.\n",
        "\n",
        "Traditionally, categorical variables such as ‘Gender’ or ‘Race’ were processed using One-hot encoding, where each entity in the variable was given a unique column to track the presence of the entity."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "916cfc2a",
      "metadata": {
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "916cfc2a"
      },
      "source": [
        "<!-- ![Screenshot%202023-02-01%20at%2019.00.55.png](attachment:Screenshot%202023-02-01%20at%2019.00.55.png) -->"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8c151c92",
      "metadata": {
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "8c151c92"
      },
      "source": [
        "This method, although convert the data into a suitable format for machine learning algorithms, it assumes independence among entity and results in sparse matrices.\n",
        "\n",
        "Entity embedding on the other hands uses a vector to represent each entity instead of a binary value. This has a few advantages;\n",
        "\n",
        "Remove the sparse matrices problem of inefficient computation\n",
        "Produce vectors that show the relationship between each entity (derive additional insights instead of treating them as independent)\n",
        " The dataset WE WILL use is the IEEE-CIS Fraud Detection data from Kaggle which you can find [here](https://www.kaggle.com/c/ieee-fraud-detection)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "800e4153",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "800e4153"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import sklearn.metrics as metrics\n",
        "from imblearn.over_sampling import SMOTE\n",
        "# define the neural networks\n",
        "from tensorflow.keras.layers import Input, Embedding, Dense, Reshape, Concatenate, Dropout, BatchNormalization\n",
        "from tensorflow.keras import Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "684dcd17",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "684dcd17"
      },
      "outputs": [],
      "source": [
        "# import pandas as pd\n",
        "train = pd.read_csv(\"Data/train_transaction.csv\")\n",
        "test = pd.read_csv(\"Data/test_transaction.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d259e3cf",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "d259e3cf",
        "outputId": "a3f687e4-0582-4de5-a373-2bbadf9dbd82"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>TransactionID</th>\n",
              "      <th>isFraud</th>\n",
              "      <th>TransactionDT</th>\n",
              "      <th>TransactionAmt</th>\n",
              "      <th>ProductCD</th>\n",
              "      <th>card1</th>\n",
              "      <th>card2</th>\n",
              "      <th>card3</th>\n",
              "      <th>card4</th>\n",
              "      <th>card5</th>\n",
              "      <th>...</th>\n",
              "      <th>V330</th>\n",
              "      <th>V331</th>\n",
              "      <th>V332</th>\n",
              "      <th>V333</th>\n",
              "      <th>V334</th>\n",
              "      <th>V335</th>\n",
              "      <th>V336</th>\n",
              "      <th>V337</th>\n",
              "      <th>V338</th>\n",
              "      <th>V339</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2987000</td>\n",
              "      <td>0</td>\n",
              "      <td>86400</td>\n",
              "      <td>68.5</td>\n",
              "      <td>W</td>\n",
              "      <td>13926</td>\n",
              "      <td>NaN</td>\n",
              "      <td>150.0</td>\n",
              "      <td>discover</td>\n",
              "      <td>142.0</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2987001</td>\n",
              "      <td>0</td>\n",
              "      <td>86401</td>\n",
              "      <td>29.0</td>\n",
              "      <td>W</td>\n",
              "      <td>2755</td>\n",
              "      <td>404.0</td>\n",
              "      <td>150.0</td>\n",
              "      <td>mastercard</td>\n",
              "      <td>102.0</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2987002</td>\n",
              "      <td>0</td>\n",
              "      <td>86469</td>\n",
              "      <td>59.0</td>\n",
              "      <td>W</td>\n",
              "      <td>4663</td>\n",
              "      <td>490.0</td>\n",
              "      <td>150.0</td>\n",
              "      <td>visa</td>\n",
              "      <td>166.0</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2987003</td>\n",
              "      <td>0</td>\n",
              "      <td>86499</td>\n",
              "      <td>50.0</td>\n",
              "      <td>W</td>\n",
              "      <td>18132</td>\n",
              "      <td>567.0</td>\n",
              "      <td>150.0</td>\n",
              "      <td>mastercard</td>\n",
              "      <td>117.0</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2987004</td>\n",
              "      <td>0</td>\n",
              "      <td>86506</td>\n",
              "      <td>50.0</td>\n",
              "      <td>H</td>\n",
              "      <td>4497</td>\n",
              "      <td>514.0</td>\n",
              "      <td>150.0</td>\n",
              "      <td>mastercard</td>\n",
              "      <td>102.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 394 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   TransactionID  isFraud  TransactionDT  TransactionAmt ProductCD  card1  \\\n",
              "0        2987000        0          86400            68.5         W  13926   \n",
              "1        2987001        0          86401            29.0         W   2755   \n",
              "2        2987002        0          86469            59.0         W   4663   \n",
              "3        2987003        0          86499            50.0         W  18132   \n",
              "4        2987004        0          86506            50.0         H   4497   \n",
              "\n",
              "   card2  card3       card4  card5  ... V330  V331  V332  V333  V334 V335  \\\n",
              "0    NaN  150.0    discover  142.0  ...  NaN   NaN   NaN   NaN   NaN  NaN   \n",
              "1  404.0  150.0  mastercard  102.0  ...  NaN   NaN   NaN   NaN   NaN  NaN   \n",
              "2  490.0  150.0        visa  166.0  ...  NaN   NaN   NaN   NaN   NaN  NaN   \n",
              "3  567.0  150.0  mastercard  117.0  ...  NaN   NaN   NaN   NaN   NaN  NaN   \n",
              "4  514.0  150.0  mastercard  102.0  ...  0.0   0.0   0.0   0.0   0.0  0.0   \n",
              "\n",
              "  V336  V337  V338  V339  \n",
              "0  NaN   NaN   NaN   NaN  \n",
              "1  NaN   NaN   NaN   NaN  \n",
              "2  NaN   NaN   NaN   NaN  \n",
              "3  NaN   NaN   NaN   NaN  \n",
              "4  0.0   0.0   0.0   0.0  \n",
              "\n",
              "[5 rows x 394 columns]"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train[\"isFraud\"].mean() #  0.03499000914417313\n",
        "train.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "22cf2dad",
      "metadata": {
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "22cf2dad"
      },
      "source": [
        "As data exploration and feature engineering is not the purpose of this post, I will use minimum features to predict the fraud label. To make sure you can replicate my code, here are my processing steps."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d27aa9e9",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "d27aa9e9"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "# generate time of day\n",
        "train[\"Time of Day\"] = np.floor(train['TransactionDT']/3600/183)\n",
        "test[\"Time of Day\"] = np.floor(test['TransactionDT']/3600/183)\n",
        "\n",
        "# drop columns\n",
        "train.drop(\"TransactionDT\",axis=1,inplace=True)\n",
        "test.drop(\"TransactionDT\",axis=1,inplace=True)\n",
        "\n",
        "# define continuous and categorical variables\n",
        "cont_vars = [\"TransactionAmt\"]\n",
        "cat_vars = ['ProductCD','addr1','addr2','P_emaildomain','R_emaildomain','Time of Day'] + [col for col in train.columns if \"card\" in col]\n",
        "\n",
        "# set training and testing set\n",
        "x_train = train[cont_vars + cat_vars].copy()\n",
        "y_train = train['isFraud'].copy()\n",
        "x_test = train[cont_vars + cat_vars].copy()\n",
        "y_test = train['isFraud'].copy()\n",
        "\n",
        "# process cont_vars\n",
        "# scale values\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "x_train['TransactionAmt'] = scaler.fit_transform(x_train['TransactionAmt'].values.reshape(-1,1))\n",
        "x_test['TransactionAmt'] = scaler.transform(x_test['TransactionAmt'].values.reshape(-1,1))\n",
        "\n",
        "# reduce cardinality of categorical variables\n",
        "idx_list = x_train['card1'].value_counts()[x_train['card1'].value_counts()<=100].index.tolist()\n",
        "x_train.loc[x_train['card1'].isin(idx_list),'card1'] = 'Others'\n",
        "x_test.loc[x_test['card1'].isin(idx_list),'card1'] = 'Others'\n",
        "\n",
        "# fill missing\n",
        "x_train[cat_vars] = x_train[cat_vars].fillna(\"Missing\")\n",
        "x_test[cat_vars] = x_test[cat_vars].fillna(\"Missing\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "76c28c54",
      "metadata": {
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "76c28c54"
      },
      "source": [
        "After the processing steps have been done, now we can convert the categorical variables into integers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5f433d96",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "5f433d96"
      },
      "outputs": [],
      "source": [
        "# convert to numerical value for modeling\n",
        "def categorify(df, cat_vars):\n",
        "    categories = {}\n",
        "    for cat in cat_vars:\n",
        "        df[cat] = df[cat].astype('category').cat.as_ordered()\n",
        "        categories[cat] = df[cat].cat.categories\n",
        "    return categories\n",
        "\n",
        "def apply_test(test,categories):\n",
        "    for cat, index in categories.items():\n",
        "        test[cat] = pd.Categorical(test[cat],categories=categories[cat],ordered=True)\n",
        "\n",
        "# convert to integers\n",
        "categories = categorify(x_train,cat_vars)\n",
        "apply_test(x_test,categories)\n",
        "\n",
        "for cat in cat_vars:\n",
        "    # print(cat.dtypes)\n",
        "    x_train[cat] = x_train[cat].astype('category').cat.codes+1 \n",
        "    x_test[cat] = x_test[cat].astype('category').cat.codes+1\n",
        "\n",
        "    # dat['street'] = dat['street_number'].astype(str).str.cat(dat['street_name'], sep=' ')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c6c61496",
      "metadata": {
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "c6c61496"
      },
      "source": [
        "Due to the higly imbalanced dataset, I have to artificially generate more fraud data using a technique called Synthetic Minority Over-sampling Technique (SMOTE). The documentation can be found here.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "caeefe93",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "caeefe93"
      },
      "outputs": [],
      "source": [
        "# from imblearn.over_sampling import SMOTE\n",
        "\n",
        "sm = SMOTE(random_state=0)\n",
        "\n",
        "x_sm, y_train = sm.fit_resample(x_train, y_train)\n",
        "x_train = pd.DataFrame(x_sm,columns=x_train.columns)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7c3820eb",
      "metadata": {
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "7c3820eb"
      },
      "source": [
        "Finally, we can work on modelling the data. This implementation of the entity embedding took the idea and best practices from Jeremy Howard’s ‘Introduction to Machine Learning for Coders’ course. Hence, many of the technical specifics such as embedding sizes and hidden layers were selected by Jeremy through his research and experiences. Anyway, if you have not seen the course, I highly recommend you take the course and listen to Jeremy approach to machine learning. It is completely free too."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "096684c7",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "096684c7"
      },
      "outputs": [],
      "source": [
        "# get embedding size for each categorical variable\n",
        "\n",
        "def get_emb_sz(cat_col,categories_dict):\n",
        "    num_classes = len(categories_dict[cat_col])\n",
        "    return int(min(600,round(1.6*num_classes**0.56)))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ccbc1777",
      "metadata": {
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "ccbc1777"
      },
      "source": [
        "Now to define the neural networks. The architecture of the networks is simply the concatenation of continuous variables with embedding layers for each categorical variables."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a84b6da0",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "a84b6da0"
      },
      "outputs": [],
      "source": [
        "# define the neural networks\n",
        "\n",
        "def combined_network(cat_vars,categories_dict,cont_vars, layers):\n",
        "    inputs = []\n",
        "    embeddings = []\n",
        "    emb_dict ={}\n",
        "\n",
        "    # create embedding layer for each categorical variable\n",
        "    for i in range(len(cat_vars)):\n",
        "        emb_dict[cat_vars[i]] = Input(shape=(1,))\n",
        "        emb_sz = get_emb_sz(cat_vars[i],categories_dict)\n",
        "        vocab = len(categories_dict[cat_vars[i]]) +1\n",
        "        embedding = Embedding(vocab,emb_sz,input_length=1)(emb_dict[cat_vars[i]])\n",
        "        embedding = Reshape(target_shape=(emb_sz,))(embedding)\n",
        "        inputs.append(emb_dict[cat_vars[i]])\n",
        "        embeddings.append(embedding)\n",
        "\n",
        "        # concat continuous variables with embedded variables\n",
        "        cont_input = Input(shape=(len(cont_vars),))\n",
        "        embedding = BatchNormalization()(cont_input)\n",
        "        inputs.append(cont_input)\n",
        "        embeddings.append(embedding)\n",
        "        x = Concatenate()(embeddings)\n",
        "\n",
        "  # add fully-connected layers separated with batchnorm and dropout layers\n",
        "  \n",
        "    for i in range(len(layers)):\n",
        "        if i ==0:\n",
        "          x = Dense(layers[i],activation='relu')(x)\n",
        "        else:\n",
        "          x = BatchNormalization()(x)\n",
        "          x = Dropout(0.5)(x)\n",
        "          x = Dense(layers[i],activation='relu')(x)\n",
        "\n",
        "    output = Dense(1,activation='sigmoid')(x)\n",
        "    model = Model(inputs,output)\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "44c29c4c",
      "metadata": {
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "44c29c4c"
      },
      "source": [
        "Now initialize the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "06a0b682",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "06a0b682"
      },
      "outputs": [],
      "source": [
        "layers = [200,100]\n",
        "\n",
        "model = combined_network(cat_vars,categories,cont_vars, layers)\n",
        "\n",
        "opt = tf.keras.optimizers.Adam(0.0001)\n",
        "model.compile(optimizer=opt,loss='binary_crossentropy',metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a721a7c2",
      "metadata": {
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "a721a7c2"
      },
      "source": [
        "Process the input for the neural networks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "32105130",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "32105130"
      },
      "outputs": [],
      "source": [
        "# process x_train input to fit model\n",
        "input_list = []\n",
        "\n",
        "for i in cat_vars:\n",
        "    input_list.append(x_train[i].values)\n",
        "    input_list.append(x_train[cont_vars].values)\n",
        "  \n",
        "# modify x_test input to fit model\n",
        "test_list = []\n",
        "\n",
        "for i in cat_vars:\n",
        "    test_list.append(x_test[i].values)\n",
        "    test_list.append(x_test[cont_vars].values)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0365a530",
      "metadata": {
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "0365a530"
      },
      "source": [
        "Train the model.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0c21fd51",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "0c21fd51",
        "outputId": "dcfc2471-041f-41c0-ab85-7ef8a3cc87be"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "35618/35618 [==============================] - 272s 7ms/step - loss: 0.3832 - accuracy: 0.8263\n",
            "Epoch 2/10\n",
            "35618/35618 [==============================] - 248s 7ms/step - loss: 0.3045 - accuracy: 0.8698\n",
            "Epoch 3/10\n",
            "35618/35618 [==============================] - 255s 7ms/step - loss: 0.2788 - accuracy: 0.8832\n",
            "Epoch 4/10\n",
            "35618/35618 [==============================] - 234s 7ms/step - loss: 0.2626 - accuracy: 0.8909\n",
            "Epoch 5/10\n",
            "35618/35618 [==============================] - 218s 6ms/step - loss: 0.2515 - accuracy: 0.8960\n",
            "Epoch 6/10\n",
            "35618/35618 [==============================] - 174s 5ms/step - loss: 0.2432 - accuracy: 0.9000\n",
            "Epoch 7/10\n",
            "35618/35618 [==============================] - 188s 5ms/step - loss: 0.2372 - accuracy: 0.9030\n",
            "Epoch 8/10\n",
            "35618/35618 [==============================] - 169s 5ms/step - loss: 0.2322 - accuracy: 0.9051\n",
            "Epoch 9/10\n",
            "35618/35618 [==============================] - 168s 5ms/step - loss: 0.2278 - accuracy: 0.9072\n",
            "Epoch 10/10\n",
            "35618/35618 [==============================] - 169s 5ms/step - loss: 0.2241 - accuracy: 0.9091\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x1e991e100>"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.fit(input_list,y_train,epochs=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d89f921b",
      "metadata": {
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "d89f921b"
      },
      "source": [
        "Making prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "406c6f4e",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "406c6f4e",
        "outputId": "77f3ae50-5d9e-4d81-fe7f-989dc0ef4194"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "18455/18455 [==============================] - 27s 1ms/step\n"
          ]
        }
      ],
      "source": [
        "y_pred = model.predict(test_list)\n",
        "# choose an optimal threshold\n",
        "y_pred = y_pred>0.1"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "876dea06",
      "metadata": {
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "876dea06"
      },
      "source": [
        "Lastly, let's rank the solution using ROC-AUC.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "107e9bb1",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "107e9bb1",
        "outputId": "70a82deb-edbd-4bd6-c223-3cc8310dd80e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.8510757063953209"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "roc = metrics.roc_auc_score(y_test,y_pred)\n",
        "roc\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e7b09171",
      "metadata": {
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "e7b09171"
      },
      "source": [
        "Not that bad for a model using only some of the features.\n",
        "\n",
        "This is it. Using entity embedding for Kaggle tabular data competition. Hope you enjoy it."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.15"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}