{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "586b5147-4b25-4ad3-af1e-21716ac8a552",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1255, 0.5377],\n",
      "        [0.6564, 0.0365],\n",
      "        [0.5837, 0.7018],\n",
      "        [0.3068, 0.9500],\n",
      "        [0.4321, 0.2946],\n",
      "        [0.6015, 0.1762],\n",
      "        [0.9945, 0.3177],\n",
      "        [0.9886, 0.3911]])\n",
      "tensor([0, 2, 2, 0, 2, 2, 0, 1])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Make data\n",
    "torch.manual_seed(4321)\n",
    "X = torch.rand(size=(8,2))\n",
    "y = torch.randint(low=0, high=3, size=(8,))\n",
    "\n",
    "print(X)\n",
    "# tensor([[0.1255, 0.5377],\n",
    "#         [0.6564, 0.0365],\n",
    "#         [0.5837, 0.7018],\n",
    "#         [0.3068, 0.9500],\n",
    "#         [0.4321, 0.2946],\n",
    "#         [0.6015, 0.1762],\n",
    "#         [0.9945, 0.3177],\n",
    "#         [0.9886, 0.3911]])\n",
    "\n",
    "print(y) \n",
    "# tensor([0, 2, 2, 0, 2, 2, 0, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8c8305a-46c5-4090-b70f-6de1860a6ae6",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "1. Initialize the weight matrices and bias vectors using `torch.tensor()`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bfa57063-83a9-4fa8-96fc-fd3856629e50",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "W0 = torch.tensor([\n",
    "    [ 0.48, -0.43],\n",
    "    [-0.51, -0.48]\n",
    "], requires_grad=True)\n",
    "W1 = torch.tensor([\n",
    "    [-0.99, 0.36, -0.75],\n",
    "    [-0.66, 0.34,  0.66]\n",
    "], requires_grad=True)\n",
    "B0 = torch.tensor([0.23, 0.05], requires_grad=True)\n",
    "B1 = torch.tensor([0.32, -0.44, 0.70], requires_grad=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7851e674-5022-457f-8c4a-04d0ef885e49",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "2. Calculate Yhat.\n",
    "\n",
    "We do this using a sequence of tensor operations mimicking the feed-forward process.\n",
    "\n",
    "First we reshape X from an (8,2) tensor into a (8,1,2) tensor so that we can perform matrix multiplication between it and W0. Then we calculate Z0, the inputs to the hidden layer activation functions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a021e515-3627-4da4-9a52-af9c1a539876",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### The estimated or predicted values in a regression or other predictive model are termed the y-hat values. \"Y\" because y is the outcome or dependent variable in the model equation, and a \"hat\" symbol (circumflex) placed over the variable name is the statistical designation of an estimated value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "794803aa-89e9-4b58-9ba5-a5477c9fd418",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.0160, -0.2621]],\n",
      "\n",
      "        [[ 0.5264, -0.2498]],\n",
      "\n",
      "        [[ 0.1522, -0.5378]],\n",
      "\n",
      "        [[-0.1073, -0.5379]],\n",
      "\n",
      "        [[ 0.2872, -0.2772]],\n",
      "\n",
      "        [[ 0.4289, -0.2932]],\n",
      "\n",
      "        [[ 0.5454, -0.5301]],\n",
      "\n",
      "        [[ 0.5051, -0.5628]]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "X0 = X.reshape(8, 1, 2)\n",
    "Z0 = X0 @ W0 + B0\n",
    "\n",
    "print(Z0)\n",
    "# tensor([[[ 0.0160, -0.2621]],\n",
    "#         [[ 0.5264, -0.2498]],\n",
    "#         [[ 0.1522, -0.5378]],\n",
    "#         [[-0.1073, -0.5379]],\n",
    "#         [[ 0.2872, -0.2772]],\n",
    "#         [[ 0.4289, -0.2932]],\n",
    "#         [[ 0.5454, -0.5301]],\n",
    "#         [[ 0.5051, -0.5628]]], grad_fn=<AddBackward0>)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66a18940-09c8-46d7-912c-a1838631fe0d",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Then we calculate the hidden layer nodes using logistic activation functions with the help of torch.exp().\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e9a33e37-2ed1-4e1b-937f-6e9b096cf50c",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.5040, 0.4349]],\n",
      "\n",
      "        [[0.6287, 0.4379]],\n",
      "\n",
      "        [[0.5380, 0.3687]],\n",
      "\n",
      "        [[0.4732, 0.3687]],\n",
      "\n",
      "        [[0.5713, 0.4311]],\n",
      "\n",
      "        [[0.6056, 0.4272]],\n",
      "\n",
      "        [[0.6331, 0.3705]],\n",
      "\n",
      "        [[0.6236, 0.3629]]], grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "A1 = 1/(1 + torch.exp(-Z0))\n",
    "\n",
    "print(A1)\n",
    "# tensor([[[0.5040, 0.4349]],\n",
    "#         [[0.6287, 0.4379]],\n",
    "#         [[0.5380, 0.3687]],\n",
    "#         [[0.4732, 0.3687]],\n",
    "#         [[0.5713, 0.4311]],\n",
    "#         [[0.6056, 0.4272]],\n",
    "#         [[0.6331, 0.3705]],\n",
    "#         [[0.6236, 0.3629]]], grad_fn=<MulBackward0>)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37d7341d-352d-4f9b-aa4b-02a70d49b103",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Then we calculate the inputs to the softmax layer.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "234cf64e-dd66-4404-8029-9a7f3f7ef1fd",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.4660, -0.1107,  0.6090]],\n",
      "\n",
      "        [[-0.5914, -0.0648,  0.5175]],\n",
      "\n",
      "        [[-0.4559, -0.1210,  0.5398]],\n",
      "\n",
      "        [[-0.3918, -0.1443,  0.5884]],\n",
      "\n",
      "        [[-0.5301, -0.0877,  0.5561]],\n",
      "\n",
      "        [[-0.5615, -0.0767,  0.5278]],\n",
      "\n",
      "        [[-0.5513, -0.0861,  0.4697]],\n",
      "\n",
      "        [[-0.5369, -0.0921,  0.4718]]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "Z1 = A1 @ W1 + B1\n",
    "\n",
    "print(Z1)\n",
    "# tensor([[[-0.4660, -0.1107,  0.6090]],\n",
    "#         [[-0.5914, -0.0648,  0.5175]],\n",
    "#         [[-0.4559, -0.1210,  0.5398]],\n",
    "#         [[-0.3918, -0.1443,  0.5884]],\n",
    "#         [[-0.5301, -0.0877,  0.5561]],\n",
    "#         [[-0.5615, -0.0767,  0.5278]],\n",
    "#         [[-0.5513, -0.0861,  0.4697]],\n",
    "#         [[-0.5369, -0.0921,  0.4718]]], grad_fn=<AddBackward0>)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d26ed019-14a1-4be8-85d9-7e147e98cc23",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Lastly, we calculate the Yhat using softmax.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2753161c-257d-4fb0-8523-e0ff94121d0f",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1867, 0.2663, 0.5470],\n",
      "        [0.1747, 0.2958, 0.5295],\n",
      "        [0.1959, 0.2738, 0.5303],\n",
      "        [0.2022, 0.2590, 0.5388],\n",
      "        [0.1812, 0.2820, 0.5368],\n",
      "        [0.1787, 0.2902, 0.5311],\n",
      "        [0.1863, 0.2966, 0.5171],\n",
      "        [0.1886, 0.2943, 0.5171]], grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "Yhat = torch.exp(Z1).squeeze() / torch.exp(Z1).sum(axis=2)\n",
    "\n",
    "print(Yhat)\n",
    "# tensor([[0.1867, 0.2663, 0.5470],\n",
    "#         [0.1747, 0.2958, 0.5295],\n",
    "#         [0.1959, 0.2738, 0.5303],\n",
    "#         [0.2022, 0.2590, 0.5388],\n",
    "#         [0.1812, 0.2820, 0.5368],\n",
    "#         [0.1787, 0.2902, 0.5311],\n",
    "#         [0.1863, 0.2966, 0.5171],\n",
    "#         [0.1886, 0.2943, 0.5171]], grad_fn=<DivBackward0>)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3134c95-4992-484a-9ca8-99808407d8d3",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "3. Calculate the categorical cross entropy loss.\n",
    "\n",
    "First we use y to index Yhat, picking out the elements that correspond to the correct class labels.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "107b2dcf-3c90-4ba5-87a5-0c7149141fc1",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.1867, 0.5295, 0.5303, 0.2022, 0.5368, 0.5311, 0.1863, 0.2943],\n",
      "       grad_fn=<IndexBackward0>)\n"
     ]
    }
   ],
   "source": [
    "p = Yhat[torch.arange(len(y)), y]\n",
    "print(p)\n",
    "# tensor([0.1867, 0.5295, 0.5303, 0.2022, 0.5368, 0.5311, 0.1863, 0.2943],\n",
    "#        grad_fn=<IndexBackward0>)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e15bf3c-5a4f-4c33-8193-ba92093715a8",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Then we calculate the loss as the negative mean of each instance loss.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "17b7494b-c2c0-4f0c-9a20-cd443a76cd8b",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.5912, grad_fn=<NegBackward0>)\n"
     ]
    }
   ],
   "source": [
    "loss = -(torch.log(p) + torch.log(1-p)).mean()\n",
    "print(loss)\n",
    "# tensor(1.5912, grad_fn=<NegBackward0>)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6de65d8-bc10-4566-8902-9547252945b5",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "4. Calculate the gradient of loss with respect to the weights and biases.\n",
    "\n",
    "Since loss is a scalar, we can simply call loss.backward(), and PyTorch will calculate the gradients for us.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ae1c3511-5be4-48d6-8616-0db26c964cdd",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[4.6499e-05, 3.3986e-02],\n",
      "        [1.3783e-02, 4.1103e-02]])\n",
      "tensor([[-0.1231, -0.0010,  0.1241],\n",
      "        [-0.0908,  0.0039,  0.0869]])\n",
      "tensor([0.0160, 0.0716])\n",
      "tensor([-0.2299,  0.0075,  0.2224])\n"
     ]
    }
   ],
   "source": [
    "loss.backward()\n",
    "\n",
    "print(W0.grad)\n",
    "# tensor([[4.6497e-05, 3.3986e-02],\n",
    "#         [1.3783e-02, 4.1103e-02]])\n",
    "\n",
    "print(W1.grad)\n",
    "# tensor([[-0.1231, -0.0010,  0.1241],\n",
    "#         [-0.0908,  0.0039,  0.0869]])\n",
    "\n",
    "print(B0.grad)\n",
    "# tensor([0.0160, 0.0716])\n",
    "\n",
    "print(B1.grad)\n",
    "# tensor([-0.2299,  0.0075,  0.2224])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f3043da-93a9-43ac-bc94-5d9bc3c22243",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "This step requires W0, W1, B0 and B1 be leaf tensors with requires_grad=True.\n",
    "\n",
    "5. All added:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5b80ef12-a842-48fa-afc6-196c9cf5fe15",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[4.6499e-05, 3.3986e-02],\n",
      "        [1.3783e-02, 4.1103e-02]])\n",
      "tensor([[-0.1231, -0.0010,  0.1241],\n",
      "        [-0.0908,  0.0039,  0.0869]])\n",
      "tensor([0.0160, 0.0716])\n",
      "tensor([-0.2299,  0.0075,  0.2224])\n"
     ]
    }
   ],
   "source": [
    "# Initialize weights and biases\n",
    "W0 = torch.tensor([\n",
    "    [ 0.48, -0.43],\n",
    "    [-0.51, -0.48]\n",
    "], requires_grad=True)\n",
    "W1 = torch.tensor([\n",
    "    [-0.99, 0.36, -0.75],\n",
    "    [-0.66, 0.34,  0.66]\n",
    "], requires_grad=True)\n",
    "B0 = torch.tensor([0.23, 0.05], requires_grad=True)\n",
    "B1 = torch.tensor([0.32, -0.44, 0.70], requires_grad=True)\n",
    "\n",
    "# Calculate Yhat\n",
    "X0 = X.reshape(8, 1, 2)\n",
    "Z0 = X0 @ W0 + B0\n",
    "A1 = 1/(1 + torch.exp(-Z0))\n",
    "Z1 = A1 @ W1 + B1\n",
    "Yhat = torch.exp(Z1).squeeze() / torch.exp(Z1).sum(axis=2)\n",
    "\n",
    "# Calculate the loss\n",
    "p = Yhat[torch.arange(len(y)), y]\n",
    "loss = -(torch.log(p) + torch.log(1-p)).mean()\n",
    "\n",
    "# Calculate the gradient\n",
    "loss.backward()\n",
    "\n",
    "print(W0.grad)\n",
    "# tensor([[4.6497e-05, 3.3986e-02],\n",
    "#         [1.3783e-02, 4.1103e-02]])\n",
    "\n",
    "print(W1.grad)\n",
    "# tensor([[-0.1231, -0.0010,  0.1241],\n",
    "#         [-0.0908,  0.0039,  0.0869]])\n",
    "\n",
    "print(B0.grad)\n",
    "# tensor([0.0160, 0.0716])\n",
    "\n",
    "print(B1.grad)\n",
    "# tensor([-0.2299,  0.0075,  0.2224])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}