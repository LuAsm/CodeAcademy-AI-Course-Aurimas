{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<a href=\"https://colab.research.google.com/github/aurimas13/CodeAcademy-AI-Course/blob/main/Notebooks_In_Progress/Deep_Learning_5_Lecture_4_Demonstration_2_of_PyTorch_Linear_Regression_blank.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ],
      "metadata": {
        "id": "jvUeXCp52d5j"
      },
      "id": "jvUeXCp52d5j"
    },
    {
      "cell_type": "markdown",
      "id": "f3b5d111",
      "metadata": {
        "id": "f3b5d111"
      },
      "source": [
        "The following additional libraries are needed to run this\n",
        "notebook where d2l package has tensorflow, keras, pytorch and everything taht is needed installed there, but this package is for presentation only."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0240d00d",
      "metadata": {
        "tags": [],
        "id": "0240d00d"
      },
      "outputs": [],
      "source": [
        "# !pip install d2l==1.0.0-beta0\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "00975c62",
      "metadata": {
        "origin_pos": 1,
        "id": "00975c62"
      },
      "source": [
        "# <b> Concise Implementation of Linear Regression </b> \n",
        "\n",
        "Deep learning has witnessed a great explosion\n",
        "of sorts over the past decade.\n",
        "The sheer number of techniques, applications and algorithms by far surpasses the\n",
        "progress of previous decades. \n",
        "This is due to a fortuitous combination of multiple factors,\n",
        "one of which is the powerful free tools\n",
        "offered by a number of open source deep learning frameworks like Theano,\n",
        "DistBelief and Caffe that  represent the\n",
        "first generation of such models \n",
        "that found widespread adoption.\n",
        "In contrast to earlier (seminal) works like\n",
        "SN2 (Simulateur Neuristique) in 1988 year\n",
        "which provided a Lisp-like programming experience,\n",
        "modern frameworks offer automatic differentiation\n",
        "and the convenience of Python.\n",
        "These frameworks allow us to automate and modularize\n",
        "the repetitive work of implementing gradient-based learning algorithms."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "41c5b0b9",
      "metadata": {
        "origin_pos": 3,
        "tab": [
          "pytorch"
        ],
        "tags": [],
        "id": "41c5b0b9"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "68a3b830",
      "metadata": {
        "origin_pos": 6,
        "id": "68a3b830"
      },
      "source": [
        "## <b> Defining the Model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1b1a584a",
      "metadata": {
        "origin_pos": 8,
        "tab": [
          "pytorch"
        ],
        "id": "1b1a584a"
      },
      "source": [
        "In PyTorch, the fully connected layer is defined in `Linear` and `LazyLinear` (available since version 1.8.0) classes. \n",
        "The latter\n",
        "allows users to *only* specify\n",
        "the output dimension,\n",
        "while the former\n",
        "additionally asks for\n",
        "how many inputs go into this layer.\n",
        "Specifying input shapes is inconvenient,\n",
        "which may require nontrivial calculations\n",
        "(such as in convolutional layers).\n",
        "Thus, for simplicity we will use such \"lazy\" layers\n",
        "whenever we can.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "97513127",
      "metadata": {
        "origin_pos": 10,
        "tab": [
          "pytorch"
        ],
        "tags": [],
        "id": "97513127"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "42ea0ae8",
      "metadata": {
        "origin_pos": 12,
        "id": "42ea0ae8"
      },
      "source": [
        "In the `forward` method, we just invoke the built-in `__call__` function of the predefined layers to compute the outputs.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "af0dc99c",
      "metadata": {
        "origin_pos": 13,
        "tab": [
          "pytorch"
        ],
        "tags": [],
        "id": "af0dc99c"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "c31ffd42",
      "metadata": {
        "origin_pos": 15,
        "id": "c31ffd42"
      },
      "source": [
        "## <b> Defining the Loss Function\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9d963701-f3ba-4ebc-aeea-510f813ec25e",
      "metadata": {
        "id": "9d963701-f3ba-4ebc-aeea-510f813ec25e"
      },
      "source": [
        "[MSELoss klasė apskaičiuoja vidutinę kvadratinę paklaidą (be puses koeficiento :eqref:eq_mse).] Pagal numatytuosius nustatymus MSELoss pateikia vidutinius nuostolius, palyginti su pavyzdžiais. Tai greiciau (ir lengviau nei koduoti viska patiems."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c87f3ebd",
      "metadata": {
        "origin_pos": 17,
        "tab": [
          "pytorch"
        ],
        "id": "c87f3ebd"
      },
      "source": [
        "`MSELoss` class returns the average loss over examples. It is faster (and easier to use) than implementing our own.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7ebe6fda",
      "metadata": {
        "origin_pos": 19,
        "tab": [
          "pytorch"
        ],
        "tags": [],
        "id": "7ebe6fda"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "0b83a1ad",
      "metadata": {
        "origin_pos": 21,
        "id": "0b83a1ad"
      },
      "source": [
        "## <b> Defining the Optimization Algorithm\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e1bd5e9e",
      "metadata": {
        "origin_pos": 23,
        "tab": [
          "pytorch"
        ],
        "id": "e1bd5e9e"
      },
      "source": [
        "Minibatch SGD is a standard tool\n",
        "for optimizing neural networks\n",
        "and thus PyTorch supports it alongside a number of\n",
        "variations on this algorithm in the `optim` module.\n",
        "When we (**instantiate an `SGD` instance,**)\n",
        "we specify the parameters to optimize over,\n",
        "obtainable from our model via `self.parameters()`,\n",
        "and the learning rate (`self.lr`)\n",
        "required by our optimization algorithm.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b0ff2902",
      "metadata": {
        "origin_pos": 25,
        "tab": [
          "pytorch"
        ],
        "tags": [],
        "id": "b0ff2902"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "7dbf9199",
      "metadata": {
        "origin_pos": 26,
        "id": "7dbf9199"
      },
      "source": [
        "## <b> Training <b>\n",
        "\n",
        "You might have noticed that expressing our model through\n",
        "high-level APIs of a deep learning framework\n",
        "requires fewer lines of code.\n",
        "We did not have to allocate parameters individually,\n",
        "define our loss function, or implement minibatch SGD.\n",
        "Once we start working with much more complex models,\n",
        "the advantages of the high-level API will grow considerably.\n",
        "Now that we have all the basic pieces in place,\n",
        "[**the training loop itself is the same\n",
        "as the one we implemented from scratch.**]\n",
        "So we just call the `fit` method\n",
        "which relies on the implementation of the `fit_epoch` method\n",
        "to train our model.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "48cda02d",
      "metadata": {
        "origin_pos": 27,
        "tab": [
          "pytorch"
        ],
        "tags": [],
        "id": "48cda02d"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "eb5a1948",
      "metadata": {
        "origin_pos": 28,
        "id": "eb5a1948"
      },
      "source": [
        "Below, we [**compare the model parameters learned\n",
        "by training on finite data\n",
        "and the actual parameters**]\n",
        "that generated our dataset.\n",
        "To access parameters,\n",
        "we access the weights and bias\n",
        "of the layer that we need.\n",
        "As in our implementation from scratch,\n",
        "note that our estimated parameters\n",
        "are close to their true counterparts.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1e4b0b65",
      "metadata": {
        "origin_pos": 29,
        "tab": [
          "pytorch"
        ],
        "tags": [],
        "id": "1e4b0b65"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eb63ab57",
      "metadata": {
        "origin_pos": 31,
        "tab": [
          "pytorch"
        ],
        "tags": [],
        "id": "eb63ab57"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "737dc694",
      "metadata": {
        "origin_pos": 32,
        "tags": [],
        "id": "737dc694"
      },
      "source": [
        "## <b> Summary <b>\n",
        "\n",
        "This demonstration showed the usage of decorators and\n",
        "implementation of a deep network\n",
        "to tap into the conveniences afforded\n",
        "by modern deep learning framework - Pytorch.\n",
        "We used this framework defaults for loading data, defining a layer,\n",
        "a loss function, an optimizer and a training loop.\n",
        "Whenever the framework provides all necessary features,\n",
        "it's generally a good idea to use them,\n",
        "since the library implementations of these components\n",
        "tend to be heavily optimized for performance, speed\n",
        "and properly tested for reliability.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "58501beb",
      "metadata": {
        "origin_pos": 34,
        "tab": [
          "pytorch"
        ],
        "id": "58501beb"
      },
      "source": [
        "In PyTorch, the `data` module provides tools for data processing,\n",
        "the `nn` module defines a large number of neural network layers and common loss functions.\n",
        "We can initialize the parameters by replacing their values\n",
        "with methods ending with `_`.\n",
        "Note that we need to specify the input dimensions of the network.\n",
        "While this is trivial for now, it can have significant knock-on effects\n",
        "when we want to design complex networks with many layers.\n",
        "Careful considerations of how to parametrize these networks\n",
        "is needed to allow portability.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.15"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}